# Data Modeling in Postgres

## Summary

A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity, and particularly understand what songs users are listening to. Unfortunately, they don't have an easy way of querying their collected data, which resides in directories of JSON logs and metadata with both user activity and songs.

In this project, Sparkify needs me to create a Postgres database to optimize queries on song play analysis. I'll need to create database schema and an ETL pipeline using Python and SQL. Helping them meet their analytical goals can improve their business outcomes.

## Libraries Used

os
glob
psycopg2
pandas

## Data Overview

### Song Dataset

The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.

### Log Dataset

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations. They're partitioned by year and month.

## Schema of Song Play Analysis

### Fact Tables

### Dimension Tables

users - users in the app
user_id, first_name, last_name, gender, level
songs - songs in music database
song_id, title, artist_id, year, duration
artists - artists in music database
artist_id, name, location, latitude, longitude
time - timestamps of records in songplays broken down into specific units
start_time, hour, day, week, month, year, weekday

## How to run the Python scripts

1) From the terminal, run create_tables.py
2) Still within the terminal, run the etl.py script
3) Run test.ipynb to confirm records were successfully inserted into each table.

#### Explanation of Files in the Repository

<b> test.ipynb </b> displays the first 5 rows of each table to let you check your database for created tables, inserted records, etc.

<b> create_tables.py </b> drops and creates tables. Run this file to reset tables prior to running an ETL script.

<b> etl.ipynb </b> reads and processes a single file from song_data and log_data and loads into selected tables using a Jupyter notebook.

<b> etl.py </b> reads and processes a single file from song_data and log_data and loads into your tables using a script.

<b> sql_queries.py </b> contains key CREATE, INSERT, and DROP sql queries.
